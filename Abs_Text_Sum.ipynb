{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cbd235",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2336d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "from tensorflow.python.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc4deb",
   "metadata": {},
   "source": [
    "## Importing the data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627b6c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './input/news-summary/news_summary_more.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8cd1d3",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c38bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate headings from read data\n",
    " \n",
    "data.drop_duplicates(subset=['headlines'],inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a752e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal from the text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converting the text to lower case\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join([contractions.fix(word) for word in text.split(\" \")])    \n",
    "    \n",
    "    tokens = [w for w in text.split() if not w in stop_words]\n",
    "    text = \" \".join(tokens)\n",
    "    text = text.replace(\"'s\",'')\n",
    "    text = text.replace(\".\",'')\n",
    "    text = re.sub(r'\\(.*\\)','',text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]',' ',text)\n",
    "    text = re.sub(r'\\.','. ',text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a25a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "_START_ upgrad learner switches career ml al 90 salary hike _END_\n",
      "Text:\n",
      "saurav kant alumnus upgrad iiit b pg program machine learning artificial intelligence sr systems engineer infosys almost 5 years work experience program upgrad 360 degree career support helped transition data scientist tech mahindra 90 salary hike upgrad online power learning powered 3 lakh careers\n",
      "\n",
      "Summary:\n",
      "_START_ delhi techie wins free food swiggy one year cred _END_\n",
      "Text:\n",
      "kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending 2000 cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cultfit more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding \"_START\" and \"_End_\" to strings for help during the tokenisation\n",
    "\n",
    "data['headlines'] = data['headlines'].apply(preprocess)\n",
    "data['text'] = data['text'].apply(preprocess)\n",
    "data['headlines'] = data['headlines'].apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "for i in range(2):\n",
    "    print('Summary:', data['headlines'][i],'Text:', data['text'][i], sep='\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4861efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of heading and text\n",
    "\n",
    "headlines_length = [len(x.split()) for x in data.headlines]\n",
    "text_length = [len(x.split()) for x in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fac2a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a graph showing length distribution of the string\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "ax1.hist(headlines_length, bins = 20)\n",
    "ax2.hist(text_length, bins = 20)\n",
    "\n",
    "ax1.title.set_text(\"Words in Headlines\")\n",
    "ax2.title.set_text(\"Words in Text\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f0fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impoting the glove data and specifing its size\n",
    "\n",
    "glove_size = 300\n",
    "f = open('./input/glove42b300dtxt/glove.42B.300d.txt', encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ab5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a multilevel embedding dictionary and calculating embedding value for each sentence\n",
    "\n",
    "embeddings_index = dict()\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b194deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the words in the corpus 3664530\n",
      "the unique words in the corpus 89954\n",
      "The number of words that are present in both glove vectors and our corpus are 70888 which is nearly 79.0% \n",
      "word 2 vec length 70888\n"
     ]
    }
   ],
   "source": [
    "# Finding the unique (rare) words in corpus and matching them against the glove vectors\n",
    "\n",
    "words_source_train = []\n",
    "for i in data['text'] :\n",
    "  words_source_train.extend(i.split(' '))\n",
    "\n",
    "print(\"all the words in the corpus\", len(words_source_train))\n",
    "words_source_train = set(words_source_train)\n",
    "print(\"the unique words in the corpus\", len(words_source_train))\n",
    "inter_words = set(embeddings_index.keys()).intersection(words_source_train)\n",
    "print(\"The number of words that are present in both glove vectors and our corpus are {} which \\\n",
    "is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))/len(words_source_train))\n",
    "*100)))\n",
    "\n",
    "words_corpus_source_train = {}\n",
    "words_glove = set(embeddings_index.keys())\n",
    "for i in words_source_train:\n",
    "  if i in words_glove:\n",
    "    words_corpus_source_train[i] = embeddings_index[i]\n",
    "print(\"word 2 vec length\", len(words_corpus_source_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea47488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'fraternityvery', 'fundn', 'finomena', 'arboozhappy', 'inswasty', 'suyun', 'dignityloving', 'pahadnikli', 'scamn', 'spiewalk', 'verenkar', 'mamaearth', 'drivehe', 'mlawas', 'evidencennnn', 'protocluster', 'rs399', 'buthero', 'shouldshow']\n"
     ]
    }
   ],
   "source": [
    "print(list(words_source_train - inter_words)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a74df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num(text):\n",
    "  words = [w for w in text.split() if not w in inter_words]\n",
    "  return len(words)\n",
    "\n",
    "data['unique_words'] = data['text'].apply(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "389e1568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    73903\n",
       "1    15826\n",
       "2     5772\n",
       "3     1973\n",
       "4      559\n",
       "5      173\n",
       "6       59\n",
       "7       11\n",
       "8        4\n",
       "Name: unique_words, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique words distribution along the sentences\n",
    "\n",
    "data['unique_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0d64c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['unique_words'] < 4]\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d59c7223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_START_ upgrad learner switches career ml al 9...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit b pg program m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_START_ delhi techie wins free food swiggy one...</td>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_START_ new zealand end rohit sharma led india...</td>\n",
       "      <td>new zealand defeated india 8 wickets fourth od...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_START_ aegon life iterm insurance plan helps ...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_START_ known hirani yrs metoo claims true son...</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97469</th>\n",
       "      <td>_START_ crpf jawan axed death maoists chhattis...</td>\n",
       "      <td>crpf jawan tuesday axed death sharp edged weap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97470</th>\n",
       "      <td>_START_ first song sonakshi sinha noor titled ...</td>\n",
       "      <td>uff yeh first song sonakshi sinha starrer upc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97471</th>\n",
       "      <td>_START_  the matrix film get reboot reports _END_</td>\n",
       "      <td>according reports new version 1999 science fic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97472</th>\n",
       "      <td>_START_ snoop dogg aims gun clown dressed trum...</td>\n",
       "      <td>new music video shows rapper snoop dogg aiming...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97473</th>\n",
       "      <td>_START_ madhesi morcha withdraws support nepal...</td>\n",
       "      <td>madhesi morcha alliance seven political partie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97474 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "0      _START_ upgrad learner switches career ml al 9...   \n",
       "1      _START_ delhi techie wins free food swiggy one...   \n",
       "2      _START_ new zealand end rohit sharma led india...   \n",
       "3      _START_ aegon life iterm insurance plan helps ...   \n",
       "4      _START_ known hirani yrs metoo claims true son...   \n",
       "...                                                  ...   \n",
       "97469  _START_ crpf jawan axed death maoists chhattis...   \n",
       "97470  _START_ first song sonakshi sinha noor titled ...   \n",
       "97471  _START_  the matrix film get reboot reports _END_   \n",
       "97472  _START_ snoop dogg aims gun clown dressed trum...   \n",
       "97473  _START_ madhesi morcha withdraws support nepal...   \n",
       "\n",
       "                                                    text  unique_words  \n",
       "0      saurav kant alumnus upgrad iiit b pg program m...             0  \n",
       "1      kunal shah credit card bill payment platform c...             2  \n",
       "2      new zealand defeated india 8 wickets fourth od...             0  \n",
       "3      aegon life iterm insurance plan customers enjo...             0  \n",
       "4      speaking sexual harassment allegations rajkuma...             0  \n",
       "...                                                  ...           ...  \n",
       "97469  crpf jawan tuesday axed death sharp edged weap...             0  \n",
       "97470   uff yeh first song sonakshi sinha starrer upc...             2  \n",
       "97471  according reports new version 1999 science fic...             0  \n",
       "97472  new music video shows rapper snoop dogg aiming...             0  \n",
       "97473  madhesi morcha alliance seven political partie...             0  \n",
       "\n",
       "[97474 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50cda7a",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9313fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spilitting the dataset across train and validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['text'], data['headlines'], test_size = 0.2, random_state = 20)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size = 0.5, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f743eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the max text and heading length\n",
    "\n",
    "max_length_x = max(text_length)\n",
    "max_length_y = max(headlines_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1baf3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokemizing the text\n",
    "\n",
    "x_t = Tokenizer()\n",
    "x_t.fit_on_texts(data['text'] + data['headlines'])\n",
    "x_vocab_size = len(x_t.word_index) + 1\n",
    "\n",
    "encoded_xtrain = x_t.texts_to_sequences(X_train)\n",
    "encoded_xval = x_t.texts_to_sequences(X_val)\n",
    "encoded_xtest = x_t.texts_to_sequences(X_test)\n",
    "\n",
    "padded_xtrain = pad_sequences(encoded_xtrain, maxlen=max_length_x, padding='post')\n",
    "padded_xval = pad_sequences(encoded_xval, maxlen=max_length_x, padding='post')\n",
    "padded_xtest = pad_sequences(encoded_xtest, maxlen=max_length_x, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37315c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Headlines\n",
    "\n",
    "y_t = Tokenizer()\n",
    "y_t.fit_on_texts(data['headlines'])\n",
    "y_vocab_size = len(y_t.word_index) + 1\n",
    "\n",
    "encoded_ytrain = y_t.texts_to_sequences(y_train)\n",
    "encoded_yval = y_t.texts_to_sequences(y_val)\n",
    "encoded_ytest = y_t.texts_to_sequences(y_test)\n",
    "\n",
    "padded_ytrain = pad_sequences(encoded_ytrain, maxlen=max_length_y, padding='post')\n",
    "padded_yval = pad_sequences(encoded_yval, maxlen=max_length_y, padding='post')\n",
    "padded_ytest = pad_sequences(encoded_ytest, maxlen=max_length_y, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3321736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedding matrix for each sentence\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((x_vocab_size, glove_size))\n",
    "for word, i in x_t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b8d03",
   "metadata": {},
   "source": [
    "#### Creating the custom attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d6d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "          \n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  \n",
    "            \n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            \n",
    "            fake_state = K.zeros_like(inputs)  \n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  \n",
    "            fake_state = K.expand_dims(fake_state)  \n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  \n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  \n",
    "\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ac6cd",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7814260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vsvik\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\vsvik\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v1.py:277: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 57, 300)      27030900    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 57, 500),    1602000     ['embedding[0][0]']              \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 57, 500),    2002000     ['lstm[0][0]']                   \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 300)    27030900    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 57, 500),    2002000     ['lstm_1[0][0]']                 \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 500),  1602000     ['embedding_1[0][0]',            \n",
      "                                 (None, 500),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 500)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 1000)   0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 34806)  34840806   ['concat_layer[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 96,611,106\n",
      "Trainable params: 42,549,306\n",
      "Non-trainable params: 54,061,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow._api.v2.compat.v1 as tf2\n",
    "tf2.disable_v2_behavior()\n",
    "\n",
    "latent_dim=500\n",
    "\n",
    "K.clear_session() \n",
    "\n",
    "encoder_inputs = Input(shape=(max_length_x,)) \n",
    "enc_emb = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False)(encoder_inputs) \n",
    "\n",
    "#LSTM \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(x_vocab_size, glove_size, weights=[embedding_matrix],input_length=max_length_x, trainable=False) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fca7599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77979 samples, validate on 9748 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Could not find variable attention_layer/V_a. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/attention_layer/V_a/class tensorflow::Var does not exist.\n\t [[{{node attention_layer/while/MatMul_2/ReadVariableOp}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1380/3222257281.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_checkpoint_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadded_xtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_ytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_ytrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_ytrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_ytrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadded_xval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_yval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_yval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_yval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_yval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m     return fit_loop(\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4184\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4186\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   4187\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Could not find variable attention_layer/V_a. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/attention_layer/V_a/class tensorflow::Var does not exist.\n\t [[{{node attention_layer/while/MatMul_2/ReadVariableOp}}]]"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "checkpoint_filepath = './model.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True, save_freq = \"epoch\")\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "history=model.fit([padded_xtrain,padded_ytrain[:,:-1]], padded_ytrain.reshape(padded_ytrain.shape[0],padded_ytrain.shape[1], 1)[:,1:] ,epochs=10,batch_size=512, validation_data=([padded_xval,padded_yval[:,:-1]], padded_yval.reshape(padded_yval.shape[0],padded_yval.shape[1], 1)[:,1:]), callbacks=[es, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b476e728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1380/2917726702.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model.27-3.27.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.load_weights(\"./model.27-3.27.h5\")\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32845cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_t.index_word \n",
    "reverse_source_word_index = x_t.index_word \n",
    "target_word_index = y_t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_length_x,latent_dim))\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097db94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    input_seq= input_seq.reshape(1,max_length_x)\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "  \n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    " \n",
    "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_length_y-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(\"Review:\",seq2text(padded_xtest[i]))\n",
    "  print(\"Original summary:\",seq2summary(padded_ytest[i]))\n",
    "  print(\"Predicted summary:\",decode_sequence(padded_xtest[i]))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4070fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_Score(y_test, y_pred):\n",
    "    references = [[seq2summary(y_test).split(\" \")]]\n",
    "    candidates = [decode_sequence(y_pred.reshape(1,max_length_x)).split(\" \")]\n",
    "    return corpus_bleu(references, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "scores=[]\n",
    "for i in range(0,500):\n",
    "    scores.append(BLEU_Score(padded_ytest[i],padded_xtest[i]))\n",
    "    \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a395c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "sentence_encoder = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f496d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(padded_xval, padded_yval):\n",
    "  scores = []\n",
    "  for i in range(len(padded_xval)):\n",
    "    \n",
    "    str1 = seq2summary(padded_yval[i])\n",
    "    str2 = decode_sequence(padded_xval[i])\n",
    "    embeddings = sentence_encoder([str1, str2]).numpy()\n",
    "    result = 1 - spatial.distance.cosine(embeddings[0], embeddings[1])\n",
    "    scores.append(result)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cosine_similarity(padded_xtest[:500],padded_ytest[:500] )\n",
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
